{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6515e903-8a4b-49de-8154-01b31453e462",
   "metadata": {},
   "source": [
    "## CÉLULA 1: CONFIGURAÇÃO DO AMBIENTE (Para o Pipeline \"Produtor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964a97d9-2e58-40b8-9e66-2c80307a467b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adicionado ao sys.path: C:\\Users\\extre\\RAG-para-pdfs\\src\n",
      "Raiz do Projeto: C:\\Users\\extre\\RAG-para-pdfs\n",
      "Lendo PDFs de: C:\\Users\\extre\\RAG-para-pdfs\\data\\input\n",
      "Salvando artefatos de 'Blocos' em: C:\\Users\\extre\\RAG-para-pdfs\\data\\output\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "\n",
    "#------------ teste gd ------------\n",
    "\n",
    "import unidecode # Para remover acentos\n",
    "import traceback\n",
    "import hashlib #  ADICIONADO PARA O HASH\n",
    "from typing import Optional \n",
    "import torch # embeddings\n",
    "\n",
    "#------------ fim teste gd ------------\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# --- Bibliotecas de Extração de PDF ---\n",
    "# (Certifique-se de que 'pdfplumber' está no seu requirements.txt)\n",
    "import pdfplumber \n",
    "\n",
    "# --- Configuração dos Caminhos ---\n",
    "# (Assumindo que este notebook está na pasta 'notebooks/')\n",
    "notebook_dir = os.getcwd() \n",
    "PROJECT_ROOT_PATH = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "SRC_PATH = os.path.join(PROJECT_ROOT_PATH, \"src\")\n",
    "\n",
    "# (Adiciona 'src/' ao path, caso as funções das equipes estejam lá)\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.append(SRC_PATH)\n",
    "    print(f\"Adicionado ao sys.path: {SRC_PATH}\")\n",
    "\n",
    "# --- Definição das Pastas ---\n",
    "PDF_INPUT_DIRECTORY = os.path.join(PROJECT_ROOT_PATH, \"data\", \"input\")\n",
    "JSON_OUTPUT_DIRECTORY = os.path.join(PROJECT_ROOT_PATH, \"data\", \"output\") # Pasta de saída\n",
    "\n",
    "# Cria a pasta 'output' se não existir\n",
    "os.makedirs(JSON_OUTPUT_DIRECTORY, exist_ok=True)\n",
    "\n",
    "print(f\"Raiz do Projeto: {PROJECT_ROOT_PATH}\")\n",
    "print(f\"Lendo PDFs de: {PDF_INPUT_DIRECTORY}\")\n",
    "print(f\"Salvando artefatos de 'Blocos' em: {JSON_OUTPUT_DIRECTORY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d27d3-bcbc-4077-be19-16c0d1175c9f",
   "metadata": {},
   "source": [
    "## CÉLULA 2 (Nova): IMPORTAÇÃO DO PIPELINE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02b880f8-ed5f-4ef2-a21b-3bb9742d8586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motor do pipeline (etapa_extracao.py) importado com sucesso.\n",
      "Motor (Etapa 1+3) importado.\n"
     ]
    }
   ],
   "source": [
    "# Importa a FUNÇÃO PRINCIPAL que criamos no arquivo etapa_extracao.py\n",
    "from etapa_extracao import processar_documento\n",
    "from etapa_tabelas import enriquecer_tabelas\n",
    "print(\"Motor do pipeline (etapa_extracao.py) importado com sucesso.\")\n",
    "print(\"Motor (Etapa 1+3) importado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7661a1-9e98-4799-9e2b-d5f6f801499d",
   "metadata": {},
   "source": [
    "## CÉLULA 3: EXECUÇÃO (O Orquestrador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc689800-3009-43fb-ae11-54eb413aeb3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando busca por PDFs em: C:\\Users\\extre\\Documents\\preprocessamento-pdfs\\data\\input\n",
      "Encontrados 7 PDFs.\n",
      "\n",
      "--- Processando: Ciencia-da-computacao-01-2025_com_sala_MkIII.pdf ---\n",
      "Iniciando particionamento com 'unstructured' (estratégia hi_res)...\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Particionamento concluído.\n",
      "Processamento concluído. JSONL salvo em: C:\\Users\\extre\\Documents\\preprocessamento-pdfs\\data\\output_blocos\\Ciencia-da-computacao-01-2025_com_sala_MkIII.jsonl\n",
      "\n",
      "--- Processando: Estatuto - Fevereiro 2025_.pdf ---\n",
      "Iniciando particionamento com 'unstructured' (estratégia hi_res)...\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Particionamento concluído.\n",
      "Processamento concluído. JSONL salvo em: C:\\Users\\extre\\Documents\\preprocessamento-pdfs\\data\\output_blocos\\Estatuto - Fevereiro 2025_.jsonl\n",
      "\n",
      "--- Processando: Estágio.pdf ---\n",
      "Iniciando particionamento com 'unstructured' (estratégia hi_res)...\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Particionamento concluído.\n",
      "Processamento concluído. JSONL salvo em: C:\\Users\\extre\\Documents\\preprocessamento-pdfs\\data\\output_blocos\\Estágio.jsonl\n",
      "\n",
      "--- Processando: PPCBCC2019.pdf ---\n",
      "Iniciando particionamento com 'unstructured' (estratégia hi_res)...\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Particionamento concluído.\n",
      "Processamento concluído. JSONL salvo em: C:\\Users\\extre\\Documents\\preprocessamento-pdfs\\data\\output_blocos\\PPCBCC2019.jsonl\n",
      "\n",
      "--- Processando: Regimento Interno dos Campi.pdf ---\n",
      "Iniciando particionamento com 'unstructured' (estratégia hi_res)...\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Particionamento concluído.\n",
      "Processamento concluído. JSONL salvo em: C:\\Users\\extre\\Documents\\preprocessamento-pdfs\\data\\output_blocos\\Regimento Interno dos Campi.jsonl\n",
      "\n",
      "--- Processando: REGIMENTO_GERAL_FEVEREIRO_DE_2025.pdf ---\n",
      "Iniciando particionamento com 'unstructured' (estratégia hi_res)...\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Particionamento concluído.\n",
      "Processamento concluído. JSONL salvo em: C:\\Users\\extre\\Documents\\preprocessamento-pdfs\\data\\output_blocos\\REGIMENTO_GERAL_FEVEREIRO_DE_2025.jsonl\n",
      "\n",
      "--- Processando: RegulamentoCursosGraduação.pdf ---\n",
      "Iniciando particionamento com 'unstructured' (estratégia hi_res)...\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Particionamento concluído.\n",
      "Processamento concluído. JSONL salvo em: C:\\Users\\extre\\Documents\\preprocessamento-pdfs\\data\\output_blocos\\RegulamentoCursosGraduação.jsonl\n",
      "\n",
      "--- Todos os PDFs foram processados. ---\n"
     ]
    }
   ],
   "source": [
    "print(f\"Iniciando busca por PDFs em: {PDF_INPUT_DIRECTORY}\")\n",
    "lista_de_pdfs = glob.glob(os.path.join(PDF_INPUT_DIRECTORY, \"*.pdf\"))\n",
    "\n",
    "if not lista_de_pdfs:\n",
    "    print(\"Nenhum PDF encontrado.\")\n",
    "else:\n",
    "    print(f\"Encontrados {len(lista_de_pdfs)} PDFs.\")\n",
    "    \n",
    "    # Loop principal que chama a lógica externa\n",
    "    for pdf_path in lista_de_pdfs:\n",
    "        \n",
    "        # Define o caminho de saída\n",
    "        doc_id_base = Path(pdf_path).stem\n",
    "        jsonl_output_path = os.path.join(\n",
    "            JSON_OUTPUT_DIRECTORY, \n",
    "            doc_id_base + \".jsonl\"\n",
    "        )\n",
    "        \n",
    "        # CHAMA A FUNÇÃO ÚNICA DE PROCESSAMENTO\n",
    "        # Todo o trabalho pesado acontece dentro desta função\n",
    "        processar_documento(pdf_path, jsonl_output_path)\n",
    "\n",
    "    print(\"\\n--- Todos os PDFs foram processados. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd9cb433-9e10-403e-a155-fdeab2300a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## aaaa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54a1850-2da1-46ba-80bc-8466b6c4391c",
   "metadata": {},
   "source": [
    "## CÉLULA 4: alocacao da tabela resumo no json (verificar)\n",
    "\n",
    "*TESTE* Gabriel Davi Normalizacao, teste da deduplicacao, funcao para metadados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e49d1266-a00c-456a-8e28-ddcccc5f89d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Módulo 'etapa_tabelas.py' importado com sucesso.\n",
      "\n",
      "--- Iniciando Etapa 4: Enriquecimento de Tabelas ---\n",
      " [Etapa 4] Processando arquivo: Ciencia-da-computacao-01-2025_com_sala_MkIII.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Concluído. 7 tabelas convertidas em 67 linhas.\n",
      " [Etapa 4] Processando arquivo: Estatuto - Fevereiro 2025_.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviso: Falha ao processar tabela HTML: No tables found matching pattern '.+'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Concluído. 5 tabelas convertidas em 704 linhas.\n",
      " [Etapa 4] Processando arquivo: Estágio.jsonl\n",
      "   -> Concluído. 0 tabelas convertidas em 248 linhas.\n",
      " [Etapa 4] Processando arquivo: PPCBCC2019.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviso: Falha ao processar tabela HTML: No tables found matching pattern '.+'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviso: Falha ao processar tabela HTML: No tables found matching pattern '.+'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviso: Falha ao processar tabela HTML: No tables found matching pattern '.+'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviso: Falha ao processar tabela HTML: No tables found matching pattern '.+'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviso: Falha ao processar tabela HTML: No tables found matching pattern '.+'\n",
      "Aviso: Falha ao processar tabela HTML: No tables found matching pattern '.+'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviso: Falha ao processar tabela HTML: No tables found matching pattern '.+'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviso: Falha ao processar tabela HTML: No tables found matching pattern '.+'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviso: Falha ao processar tabela HTML: No tables found matching pattern '.+'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviso: Falha ao processar tabela HTML: No tables found matching pattern '.+'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviso: Falha ao processar tabela HTML: No tables found matching pattern '.+'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviso: Falha ao processar tabela HTML: No tables found matching pattern '.+'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviso: Falha ao processar tabela HTML: No tables found matching pattern '.+'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Concluído. 166 tabelas convertidas em 2395 linhas.\n",
      " [Etapa 4] Processando arquivo: Regimento Interno dos Campi.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviso: Falha ao processar tabela HTML: No tables found matching pattern '.+'\n",
      "Aviso: Falha ao processar tabela HTML: No tables found matching pattern '.+'\n",
      "Aviso: Falha ao processar tabela HTML: No tables found matching pattern '.+'\n",
      "   -> Concluído. 4 tabelas convertidas em 978 linhas.\n",
      " [Etapa 4] Processando arquivo: REGIMENTO_GERAL_FEVEREIRO_DE_2025.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Concluído. 7 tabelas convertidas em 2779 linhas.\n",
      " [Etapa 4] Processando arquivo: RegulamentoCursosGraduação.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Concluído. 4 tabelas convertidas em 1265 linhas.\n",
      "--- Etapa 4 (Tabelas) Concluída ---\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from etapa_tabelas import enriquecer_tabelas\n",
    "    print(\"Módulo 'etapa_tabelas.py' importado com sucesso.\")\n",
    "except ImportError as e:\n",
    "    print(\"!!! ERRO DE IMPORTAÇÃO !!!\")\n",
    "    print(\"Verifique se o arquivo 'etapa_tabelas.py' está na pasta 'src/'\")\n",
    "    print(f\"Erro: {e}\")\n",
    "    # Define um placeholder\n",
    "    def enriquecer_tabelas(directory):\n",
    "        print(\"ERRO: A função 'enriquecer_tabelas' não foi importada!\")\n",
    "\n",
    "enriquecer_tabelas(JSON_OUTPUT_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12969a92-1874-4ab8-8104-605685c4969f",
   "metadata": {},
   "source": [
    "## CÉLULA 5: Normalização do texto bruto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5fddf0d-989b-42f8-acd4-fb60f9cd4283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Normalização iniciada ---\n",
      "-> Aplicando normalização (lowercase) nos dicionários carregados...\n",
      "-> Dicionários de normalização carregados de 'C:\\Users\\lucas\\Documents\\Faculdade\\Projeto Integrador\\PipelineRag 1.9\\preprocessamento-pdfs\\data\\input\\dicionarios.json'.\n",
      "\n",
      "--- Iniciando Normalização de Texto ---\n",
      " [Etapa Normalização] Normalizando arquivo: Estágio.jsonl\n",
      "   -> Concluído. 248 textos normalizados em 248 linhas.\n",
      " [Etapa Normalização] Normalizando arquivo: PPCBCC2019.jsonl\n",
      "   -> Concluído. 2386 textos normalizados em 2386 linhas.\n",
      "--- Normalização Concluída ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Normalização iniciada ---\")\n",
    "\n",
    "def normalize_text(text: str, acronyms_map: dict, standardization_map: dict) -> str:\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"    \n",
    "    # Vai converte para minúsculas e Remove acentos.\n",
    "    text = text.lower()  \n",
    "    text = unidecode.unidecode(text)\n",
    "    \n",
    "    # --- MUDANÇA PRINCIPAL AQUI ---\n",
    "    # Expansão de Siglas (ex: \"ifnmg\" -> \"ifnmg instituto federal do norte de minas gerais\").\n",
    "    # (Agora, ele mantém a sigla E adiciona a expansão)\n",
    "    for acronym, expansion in acronyms_map.items():\n",
    "        # O 'replacement' agora é a própria sigla + a expansão\n",
    "        replacement_string = f\"{acronym} {expansion}\"\n",
    "        text = re.sub(r'\\b' + re.escape(acronym) + r'\\b', replacement_string, text)\n",
    "    # --- FIM DA MUDANÇA ---\n",
    "        \n",
    "    # Padronização  (ex: \"ppc\" -> \"projeto pedagogico de curso\").\n",
    "    for key, value in standardization_map.items():\n",
    "        # Usa regex para garantir que estamos substituindo a palavra inteira.\n",
    "        text = re.sub(r'\\b' + re.escape(key) + r'\\b', value, text)\n",
    "\n",
    "    # Remove caracteres especiais.\n",
    "    text = re.sub(r'[^a-z0-9\\s.,-]', '', text)\n",
    "    \n",
    "    # Remove espaços extras.\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def executar_normalizacao(jsonl_directory: str, acronyms: dict, standardization: dict):\n",
    "    print(f\"\\n--- Iniciando Normalização de Texto ---\")\n",
    "    \n",
    "    jsonl_files = glob.glob(os.path.join(jsonl_directory, \"*.jsonl\"))\n",
    "    if not jsonl_files:\n",
    "        print(f\" [Etapa Normalização] Nenhum arquivo .jsonl encontrado em {jsonl_directory} para normalizar.\")\n",
    "        return\n",
    "\n",
    "    for file_path in jsonl_files:\n",
    "        print(f\" [Etapa Normalização] Normalizando arquivo: {os.path.basename(file_path)}\")\n",
    "        temp_file_path = file_path + \".temp\"\n",
    "        linhas_processadas = 0\n",
    "        textos_normalizados = 0\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f_in, \\\n",
    "                 open(temp_file_path, 'w', encoding='utf-8') as f_out:\n",
    "                \n",
    "                for line in f_in:\n",
    "                    linhas_processadas += 1\n",
    "                    try:\n",
    "                        data = json.loads(line)\n",
    "                        \n",
    "                        # Força o reprocessamento limpando a chave antiga\n",
    "                        data[\"texto_normalizado\"] = None\n",
    "                        \n",
    "                        # Pula se não houver texto bruto\n",
    "                        if not data.get(\"texto_bruto\"):\n",
    "                            f_out.write(line)\n",
    "                            continue\n",
    "\n",
    "                        texto_bruto = data[\"texto_bruto\"]\n",
    "                        texto_norm = normalize_text(texto_bruto, acronyms, standardization)\n",
    "                        \n",
    "                        data[\"texto_normalizado\"] = texto_norm\n",
    "                        \n",
    "                        f_out.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "                        textos_normalizados += 1\n",
    "                        \n",
    "                    except json.JSONDecodeError:\n",
    "                        f_out.write(line)\n",
    "            \n",
    "            os.replace(temp_file_path, file_path)\n",
    "            print(f\"   -> Concluído. {textos_normalizados} textos normalizados em {linhas_processadas} linhas.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"!!! ERRO FATAL [Etapa 2] ao processar {file_path}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            if os.path.exists(temp_file_path):\n",
    "                os.remove(temp_file_path)\n",
    "\n",
    "    print(\"--- Normalização Concluída ---\")\n",
    "\n",
    "# Carregamento dos Dicionários e Execução ---\n",
    "dicionarios_path = os.path.join(PDF_INPUT_DIRECTORY, 'dicionarios.json')\n",
    "acronyms = {}\n",
    "standardization_map = {}\n",
    "\n",
    "try:\n",
    "    with open(dicionarios_path, 'r', encoding='utf-8') as f:\n",
    "        dictionaries = json.load(f)\n",
    "    \n",
    "    acronyms_original = dictionaries.get(\"acronyms\", {})\n",
    "    standardization_map_original = dictionaries.get(\"standardization_map\", {})\n",
    "\n",
    "    print(\"-> Aplicando normalização (lowercase) nos dicionários carregados...\")\n",
    "    acronyms = {k.lower(): v.lower() for k, v in acronyms_original.items()}\n",
    "    standardization_map = {k.lower(): v.lower() for k, v in standardization_map_original.items()}\n",
    "    \n",
    "    print(f\"-> Dicionários de normalização carregados de '{dicionarios_path}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"-> AVISO: '{dicionarios_path}' não encontrado. Usando dicionários vazios.\")\n",
    "\n",
    "executar_normalizacao(JSON_OUTPUT_DIRECTORY, acronyms, standardization_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297d9b8a-8aab-458a-812e-d3a7538cf0e5",
   "metadata": {},
   "source": [
    "## CÉLULA 6 Deduplicação de Julia adaptada ao novo pipeline\n",
    "\n",
    "### NOTA ANTES DA DEDUPLIÇÃO:\n",
    "\n",
    "                            --- COMPARATIVO COM A EQUIPE ANTERIOR ---\n",
    "                        Comparativo com a abordagem da equipe anterior:\n",
    "                        Ambos os scripts visam uma deduplicação semântica e cruzada (entre documentos). \n",
    "                        Para isso, adotamos os mesmos critérios de negócios que eles estabeleceram: \n",
    "                            1) Um limite de similaridade de 85% (threshold=0.85).\n",
    "                            2) Uma regra para ignorar textos curtos (min_length=50).\n",
    "                        A principal diferença é a troca da tecnologia e a adaptação ao nosso pipeline: \n",
    "                        substituímos o 'sklearn' (TF-IDF) por 'sentence- transformers' (Embeddings), que é mais \n",
    "                        preciso semanticamente, e adaptamos a lógica para operar em nosso formato de dados .jsonl. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "301280bb-0f57-425c-8e97-11b2b5e3d17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Módulo 'deduplicate.py' (Semântico) importado com sucesso.\n",
      "\n",
      "--- Iniciando Etapa 3: Deduplicação SEMÂNTICA (Threshold=95.0%, Min-Length=50 chars) ---\n",
      "   -> Carregando modelo de embedding: 'sentence-transformers/all-MiniLM-L6-v2'...\n",
      "      (Isso pode demorar um pouco na primeira vez)\n",
      "      (Usando dispositivo: cpu)\n",
      "Ordem de processamento: ['Estágio.jsonl', 'PPCBCC2019.jsonl']\n",
      "\n",
      " [Etapa 3] Processando semanticamente: Estágio.jsonl\n",
      "   (Embeddings únicos em memória antes: 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d57995a9154d21b92adfed13700b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> (Primeiro arquivo, todos os blocos são únicos por padrão)\n",
      "   -> Concluído. Blocos: 248 -> 248 (0 removidos)\n",
      "   (Embeddings únicos em memória agora: 183)\n",
      "\n",
      " [Etapa 3] Processando semanticamente: PPCBCC2019.jsonl\n",
      "   (Embeddings únicos em memória antes: 183)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe44122cacf42f1a01bf8e7795199b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Concluído. Blocos: 2386 -> 2386 (0 removidos)\n",
      "   (Embeddings únicos em memória agora: 1329)\n",
      "\n",
      "--- Etapa 3 (Deduplicação SEMÂNTICA) Concluída ---\n",
      "Total de blocos antes: 2634\n",
      "Total de blocos depois: 2634 (0 removidos no total)\n",
      "Em caso de '0 removidos' considere que o texto ja tinha sido limpo e nao foi extraido novamente\n"
     ]
    }
   ],
   "source": [
    "# CÉLULA 6 deduplicacao\n",
    "\n",
    "try:\n",
    "    from deduplicate import get_semantic_model, deduplicate_semantically\n",
    "    print(\"Módulo 'deduplicate.py' (Semântico) importado com sucesso.\")\n",
    "except ImportError as e:\n",
    "    print(\"!!! ERRO DE IMPORTAÇÃO !!!\")\n",
    "    print(\"Verifique se o arquivo 'deduplicate.py' está na pasta 'src/'\")\n",
    "    print(f\"Erro: {e}\")\n",
    "    def get_semantic_model():\n",
    "        print(\"ERRO: get_semantic_model não importado.\")\n",
    "        return None\n",
    "    def deduplicate_semantically(blocks, **kwargs):\n",
    "        print(\"ERRO: deduplicate_semantically não importado.\")\n",
    "        return blocks, []\n",
    "\n",
    "def executar_deduplicacao_semantica(jsonl_directory: str, threshold: float, min_length: int):\n",
    "    \"\"\"\n",
    "    Orquestra o processo de deduplicação semântica.\n",
    "    Carrega o modelo 1 vez, e passa o cache de embeddings\n",
    "    de arquivo para arquivo.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Iniciando Etapa 3: Deduplicação SEMÂNTICA (Threshold={threshold*100}%, Min-Length={min_length} chars) ---\")\n",
    "    \n",
    "    # --- Carrega o modelo de IA (só uma vez) ---\n",
    "    try:\n",
    "        model = get_semantic_model()\n",
    "        if model is None: raise Exception(\"Modelo não carregou.\")\n",
    "    except Exception as e:\n",
    "        print(f\"!!! ERRO FATAL: Não foi possível carregar o modelo SentenceTransformer.\")\n",
    "        print(\"    Verifique sua instalação do 'sentence-transformers' e 'pytorch'.\")\n",
    "        print(f\"    Erro: {e}\")\n",
    "        return\n",
    "\n",
    "    jsonl_files = glob.glob(os.path.join(jsonl_directory, \"*.jsonl\"))\n",
    "    if not jsonl_files:\n",
    "        print(f\" [Etapa 3] Nenhum arquivo .jsonl encontrado em {jsonl_directory}.\")\n",
    "        return\n",
    "\n",
    "    # --- O cache global de embeddings é criado aqui ---\n",
    "    global_seen_embeddings = [] \n",
    "\n",
    "    total_blocos_antes = 0\n",
    "    total_blocos_depois = 0\n",
    "\n",
    "    jsonl_files.sort() \n",
    "    print(f\"Ordem de processamento: {[os.path.basename(f) for f in jsonl_files]}\")\n",
    "\n",
    "    for file_path in jsonl_files:\n",
    "        print(f\"\\n [Etapa 3] Processando semanticamente: {os.path.basename(file_path)}\")\n",
    "        print(f\"   (Embeddings únicos em memória antes: {len(global_seen_embeddings)})\")\n",
    "        \n",
    "        all_blocks = []\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        all_blocks.append(json.loads(line))\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Aviso: Linha corrompida pulada em {file_path}\")\n",
    "            \n",
    "            if not all_blocks:\n",
    "                print(\" -> Arquivo vazio. Pulando.\")\n",
    "                continue\n",
    "\n",
    "            # --- CHAMA A FUNÇÃO DO 'src/deduplicate.py' ---\n",
    "            clean_blocks, global_seen_embeddings = deduplicate_semantically(\n",
    "                all_blocks, \n",
    "                model=model,\n",
    "                global_seen_embeddings=global_seen_embeddings, \n",
    "                threshold=threshold, \n",
    "                min_length=min_length\n",
    "            )\n",
    "            \n",
    "            num_antes = len(all_blocks)\n",
    "            num_depois = len(clean_blocks)\n",
    "            num_removidos = num_antes - num_depois\n",
    "            total_blocos_antes += num_antes\n",
    "            total_blocos_depois += num_depois\n",
    "\n",
    "            print(f\"   -> Concluído. Blocos: {num_antes} -> {num_depois} ({num_removidos} removidos)\")\n",
    "            print(f\"   (Embeddings únicos em memória agora: {len(global_seen_embeddings)})\")\n",
    "\n",
    "            # --- SOBRESCREVER O ARQUIVO COM OS DADOS LIMPOS ---\n",
    "            temp_file_path = file_path + \".temp\"\n",
    "            with open(temp_file_path, 'w', encoding='utf-8') as f_out:\n",
    "                for block in clean_blocks:\n",
    "                    f_out.write(json.dumps(block, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "            os.replace(temp_file_path, file_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! ERRO FATAL [Etapa 3] ao processar {file_path}: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    print(\"\\n--- Etapa 3 (Deduplicação SEMÂNTICA) Concluída ---\")\n",
    "    print(f\"Total de blocos antes: {total_blocos_antes}\")\n",
    "    print(f\"Total de blocos depois: {total_blocos_depois} ({total_blocos_antes - total_blocos_depois} removidos no total)\")\n",
    "    print(\"Em caso de '0 removidos' considere que o texto ja tinha sido limpo e nao foi extraido novamente\")\n",
    "\n",
    "# Usando os critérios da equipe anterior (semântico)\n",
    "executar_deduplicacao_semantica(JSON_OUTPUT_DIRECTORY, threshold=0.95, min_length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c166093-19bc-4090-a76a-9e2d8243b1c8",
   "metadata": {},
   "source": [
    "## CÉLULA 7: Metaadadooooooos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ec28141-4707-4849-8e26-277aefde1204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Célula 7 (Metadados Híbridos v7) pronta.\n",
      "\n",
      "--- Iniciando : Enriquecimento AUTOMÁTICO de Metadados ---\n",
      " Processando arquivo: Estágio.jsonl\n",
      "   -> Metadados inferidos: {'data_documento': '2023-05', 'tipo_documento': 'Institucional', 'hash_arquivo': 'da893c5a458fcbc28d4207606fb45526'}\n",
      "   -> Concluído. 248 linhas atualizadas com metadados.\n",
      " Processando arquivo: PPCBCC2019.jsonl\n",
      "   -> Metadados inferidos: {'data_documento': '2019', 'tipo_documento': 'PPC', 'curso': 'Ciencia da Computacao', 'hash_arquivo': 'ffd0d724c95aee97e5c7fc4ff60a3eec'}\n",
      "   -> Concluído. 2386 linhas atualizadas com metadados.\n",
      "--- Metadados Automáticos concluída ---\n"
     ]
    }
   ],
   "source": [
    "print(\"Célula 7 (Metadados Híbridos v7) pronta.\")\n",
    "\n",
    "METADATOS_PADRAO = {\n",
    "    \"instituicao\": \"IFNMG\",\n",
    "    \"campus\": \"Montes Claros\",\n",
    "    \"fonte\": \"Documentos Oficiais\"\n",
    "}\n",
    "# quant blocos do início do arquivo devemos ler para \"caçar\" a data\n",
    "N_BLOCKS_TO_SCAN = 20\n",
    "\n",
    "def _calculate_file_hash(file_path: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Calcula o hash MD5 de um arquivo.\n",
    "    Retorna o hash em hexadecimal ou None se o arquivo não for encontrado.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            file_hash = hashlib.md5()\n",
    "            while chunk := f.read(8192):\n",
    "                file_hash.update(chunk)\n",
    "        return file_hash.hexdigest()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"   -> Aviso: PDF original '{file_path}' não encontrado para calcular o hash.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"   -> Erro ao calcular hash para '{file_path}': {e}\")\n",
    "        return None\n",
    "\n",
    "# --- FUNÇÃO DE INFERÊNCIA ---\n",
    "def infer_metadata_from_content(blocks: List[Dict[str, Any]], file_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Analisa os primeiros blocos de um .jsonl E o nome do arquivo\n",
    "    para extrair metadados (incluindo a data de precisão variável).\n",
    "    \"\"\"\n",
    "    \n",
    "    file_name_lower = file_name.lower()\n",
    "    inferred_data = {}\n",
    "    \n",
    "    month_map = {\n",
    "        'janeiro': '01', 'fevereiro': '02', 'marco': '03', 'abril': '04',\n",
    "        'maio': '05', 'junho': '06', 'julho': '07', 'agosto': '08',\n",
    "        'setembro': '09', 'outubro': '10', 'novembro': '11', 'dezembro': '12'\n",
    "    }\n",
    "    \n",
    "    # --- Padrões de Regex para \"Caçar\" a Data ---\n",
    "    date_pattern_full = re.compile(r'(\\d{1,2})\\s+de\\s+(' + '|'.join(month_map.keys()) + r')\\s+de\\s+(\\d{4})')\n",
    "    date_pattern_month_year = re.compile(r'(' + '|'.join(month_map.keys()) + r')\\s+de\\s+(\\d{4})')\n",
    "    date_pattern_numeric = re.compile(r'(\\d{1,2})/(\\d{1,2})/(\\d{4})')\n",
    "    \n",
    "    found_date = False\n",
    "    \n",
    "    # \"Caçar\" a data no CONTEÚDO (primeiros N blocos) ---\n",
    "    for block in blocks[:N_BLOCKS_TO_SCAN]:\n",
    "        text_to_scan = block.get(\"texto_bruto\", \"\").lower()\n",
    "        \n",
    "        # Regra 1: \"18 de junho de 2007\" -> YYYY-MM-DD\n",
    "        match = date_pattern_full.search(text_to_scan)\n",
    "        if match:\n",
    "            day = match.group(1).zfill(2)\n",
    "            month = month_map[match.group(2)]\n",
    "            year = match.group(3)\n",
    "            inferred_data[\"data_documento\"] = f\"{year}-{month}-{day}\"\n",
    "            found_date = True\n",
    "            break\n",
    "\n",
    "        # Regra 2: \"15/02/2025\" -> YYYY-MM-DD\n",
    "        match = date_pattern_numeric.search(text_to_scan)\n",
    "        if match:\n",
    "            day = match.group(1).zfill(2)\n",
    "            month = match.group(2).zfill(2)\n",
    "            year = match.group(3)\n",
    "            inferred_data[\"data_documento\"] = f\"{year}-{month}-{day}\"\n",
    "            found_date = True\n",
    "            break\n",
    "\n",
    "        # Regra 3: \"Fevereiro de 2025\" -> YYYY-MM\n",
    "        match = date_pattern_month_year.search(text_to_scan)\n",
    "        if match:\n",
    "            month = month_map[match.group(1)]\n",
    "            year = match.group(2)\n",
    "            inferred_data[\"data_documento\"] = f\"{year}-{month}\"\n",
    "            found_date = True\n",
    "            break\n",
    "\n",
    "    # --- Fallback (Usar o NOME DO ARQUIVO) ---\n",
    "    if not found_date:\n",
    "        year_match = re.search(r'(20\\d{2})', file_name_lower)\n",
    "        if year_match:\n",
    "            year = year_match.group(1)\n",
    "            inferred_data[\"data_documento\"] = year\n",
    "        else:\n",
    "            inferred_data[\"data_documento\"] = None \n",
    "\n",
    "    if \"ppc\" in file_name_lower:\n",
    "        inferred_data[\"tipo_documento\"] = \"PPC\"\n",
    "    elif \"estatuto\" in file_name_lower:\n",
    "        inferred_data[\"tipo_documento\"] = \"Estatuto\"\n",
    "    elif \"horario\" in file_name_lower or \"sala\" in file_name_lower:\n",
    "        inferred_data[\"tipo_documento\"] = \"Horario Escolar\"\n",
    "    elif \"regulamento\" in file_name_lower:\n",
    "        inferred_data[\"tipo_documento\"] = \"Regulamento\"\n",
    "    else:\n",
    "        inferred_data[\"tipo_documento\"] = \"Institucional\" \n",
    "\n",
    "    # --- Inferir o Curso (do nome do arquivo) ---\n",
    "    if \"computacao\" in file_name_lower or \"bcc\" in file_name_lower:\n",
    "        inferred_data[\"curso\"] = \"Ciencia da Computacao\"\n",
    "    elif \"engenharia\" in file_name_lower:\n",
    "        inferred_data[\"curso\"] = \"Engenharia\"\n",
    "        \n",
    "    return inferred_data\n",
    "\n",
    "# --- Orquestrador (Atualizado para ler blocos primeiro) ---\n",
    "def executar_enriquecimento_metadata(jsonl_directory: str, default_metadata: dict):\n",
    "    \"\"\"\n",
    "    Lê cada .jsonl, calcula o hash do PDF original, \n",
    "    infere metadados (caçando a data no conteúdo) e salva o arquivo atualizado.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Iniciando : Enriquecimento AUTOMÁTICO de Metadados ---\")\n",
    "    \n",
    "    jsonl_files = glob.glob(os.path.join(jsonl_directory, \"*.jsonl\"))\n",
    "    if not jsonl_files:\n",
    "        print(f\" Nenhum arquivo .jsonl encontrado em {jsonl_directory}.\")\n",
    "        return\n",
    "\n",
    "    for file_path in jsonl_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        print(f\" Processando arquivo: {file_name}\")\n",
    "        \n",
    "        # --- LÓGICA DE HASH ADICIONADA ---\n",
    "        pdf_name = Path(file_name).stem + \".pdf\"\n",
    "        pdf_path = os.path.join(PDF_INPUT_DIRECTORY, pdf_name)\n",
    "        file_hash = _calculate_file_hash(pdf_path)\n",
    "        # --- FIM DA LÓGICA DE HASH ---\n",
    "        \n",
    "        all_blocks = []\n",
    "        try:\n",
    "            # --- ETAPA 1: LER TODOS OS BLOCOS PARA MEMÓRIA ---\n",
    "            with open(file_path, 'r', encoding='utf-8') as f_in:\n",
    "                for line in f_in:\n",
    "                    try:\n",
    "                        all_blocks.append(json.loads(line))\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Aviso: Linha corrompida pulada em {file_name}\")\n",
    "\n",
    "            if not all_blocks:\n",
    "                print(\"   -> Arquivo vazio. Pulando.\")\n",
    "                continue\n",
    "            \n",
    "            # --- INFERÊNCIA ---\n",
    "            # Pega metadados base da primeira linha\n",
    "            base_metadata = all_blocks[0].get(\"metadados_doc\", {})\n",
    "            custom_metadata = infer_metadata_from_content(all_blocks, file_name)\n",
    "            \n",
    "            # Adiciona o hash aos metadados\n",
    "            custom_metadata[\"hash_arquivo\"] = file_hash #\n",
    "            \n",
    "            print(f\"   -> Metadados inferidos: {custom_metadata}\")\n",
    "\n",
    "            # --- MONTAGEM FINAL E LIMPEZA ---\n",
    "            final_metadata = base_metadata\n",
    "            final_metadata.update(default_metadata)\n",
    "            final_metadata.update(custom_metadata)\n",
    "            \n",
    "            # Limpa chaves antigas (se existirem)\n",
    "            if \"ano_documento\" in final_metadata:\n",
    "                del final_metadata[\"ano_documento\"]\n",
    "            # Limpa o \"ano\" antigo, pois \"data_documento\" o substitui\n",
    "            if \"ano\" in final_metadata:\n",
    "                 del final_metadata[\"ano\"]\n",
    "            \n",
    "            # --- SALVAR ARQUIVO ---\n",
    "            temp_file_path = file_path + \".temp\"\n",
    "            linhas_processadas = 0\n",
    "            with open(temp_file_path, 'w', encoding='utf-8') as f_out:\n",
    "                for block in all_blocks:\n",
    "                    linhas_processadas += 1\n",
    "                    # Aplica os mesmos metadados em TODOS os blocos\n",
    "                    block[\"metadados_doc\"] = final_metadata \n",
    "                    f_out.write(json.dumps(block, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "            os.replace(temp_file_path, file_path)\n",
    "            print(f\"   -> Concluído. {linhas_processadas} linhas atualizadas com metadados.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"!!! ERRO FATAL ao processar {file_path}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            if os.path.exists(temp_file_path):\n",
    "                os.remove(temp_file_path)\n",
    "\n",
    "    print(\"--- Metadados Automáticos concluída ---\")\n",
    "\n",
    "executar_enriquecimento_metadata(JSON_OUTPUT_DIRECTORY, METADATOS_PADRAO)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
